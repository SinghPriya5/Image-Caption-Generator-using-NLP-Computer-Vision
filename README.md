# üñºÔ∏è Image Captioning using CNN-RNN (DenseNet201 + LSTM)

## Table of Contents
- [Overview](#overview)
- [Tech Stack](#tech-stack)
- [Dataset Used](#dataset-used)
- [Model Architecture](#model-architecture)
- [How to Run the Project](#how-to-run-the-project)
- [Predictions](#predictions)
- [Evaluation Metrics](#evaluation-metrics)
- [Conclusion](#conclusion)
- [Future Scope](#future-scope)
- [Credits](#credits)

## Overview
This project is an image captioning system that uses a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) to generate textual descriptions of images. It allows users to upload an image and get a meaningful caption generated by the deep learning model.

## Tech Stack
- Python üêç
- TensorFlow üîß
- Keras üîß
- Streamlit üåê
- DenseNet201 üì∑ (CNN)
- LSTM üß† (RNN)

## Dataset Used
- **Flickr8k Dataset**: Contains 8,000 images each paired with 5 different captions. This dataset is ideal for training and evaluating image captioning models.

## Model Architecture
- **Feature Extractor**: DenseNet201 pre-trained on ImageNet is used to extract features from images.
- **Text Generator**: LSTM layers are used to generate sequences (captions) based on extracted image features.
- **Tokenizer**: Converts words to numeric tokens to be processed by LSTM.

## How to Run the Project
1. Clone the repository:
   ```bash
   git clone https://github.com/your-username/image-caption-generator.git
   cd image-caption-generator
   ```
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Run Streamlit app:
   ```bash
   streamlit run app.py
   ```

## Predictions
Here are some sample predictions:

| Image | Predicted Caption |
|-------|-------------------|
| ![Image1](https://github.com/SinghPriya5/Image-Caption-Generator-using-NLP-Computer-Vision/blob/main/1.png) | **Two dogs are running through the water** |
| ![Image2](https://github.com/SinghPriya5/Image-Caption-Generator-using-NLP-Computer-Vision/blob/main/2.png) | **Man is standing top of the mountain** |
| ![Image3](https://github.com/SinghPriya5/Image-Caption-Generator-using-NLP-Computer-Vision/blob/main/3.png) | **Boy is jumping over the air** |

## Evaluation Metrics
To evaluate the performance of the model, the following metrics were used:
- **BLEU Score**: Measures the similarity between generated and actual captions.
- **Loss Function**: Categorical Crossentropy for text generation.
- **Accuracy**: Captures the matching of words between predicted and true sequences.

## Conclusion
This image captioning model effectively interprets the content of images and generates human-like descriptive captions. It demonstrates the potential of combining CNNs and RNNs for solving vision-language tasks.

## Future Scope
- Improve caption quality using transformers like Vision Transformer + GPT.
- Train on larger datasets like MSCOCO for more robust results.
- Add attention mechanism for better context understanding.
- Deploy as a web app for real-time image captioning with interactive image uploads via Streamlit.
- Add support for multiple languages in generated captions.
- Implement caption filtering to remove generic or repetitive captions.
- Introduce personalized captioning based on user preferences or styles.

## Credits
- Dataset: [Flickr8k](https://www.kaggle.com/datasets/adityajn105/flickr8k)
- Pre-trained CNN: DenseNet201 (via Keras Applications)
- Inspiration: Deep Learning Specialization (Coursera), Show and Tell: A Neural Image Caption Generator

